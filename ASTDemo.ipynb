{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a3db84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, Audio, ClassLabel, Features\n",
    "import torch\n",
    "from transformers import ASTConfig, ASTForAudioClassification, ASTFeatureExtractor, TrainingArguments, Trainer\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05119f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"label_map.json\", \"r\") as f:\n",
    "    label2id = json.load(f)\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce7d68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': './Dataset/genres/blues\\\\blues.00000.wav', 'array': array([ 0.00732422,  0.01660156,  0.00762939, ..., -0.05560303,\n",
      "       -0.06106567, -0.06417847]), 'sampling_rate': 22050}, 'labels': 0} {'audio': {'path': './Dataset/genres/rock\\\\rock.00099.wav', 'array': array([-0.02111816, -0.03451538, -0.03536987, ...,  0.00134277,\n",
      "        0.00250244, -0.00186157]), 'sampling_rate': 22050}, 'labels': 9}\n"
     ]
    }
   ],
   "source": [
    "class_labels = ClassLabel(names=[id2label[i] for i in range(len(id2label))])\n",
    "features = Features({\n",
    "    \"audio\": Audio(),\n",
    "    \"labels\": class_labels\n",
    "})\n",
    "\n",
    "def collect_data(root_dir=\"./Dataset/genres/\", label_map_path=\"label_map.json\"):\n",
    "    data = {\"audio\": [], \"labels\": []}\n",
    "    for label in sorted(os.listdir(root_dir)):\n",
    "        class_dir = os.path.join(root_dir, label)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                filepath = os.path.join(class_dir, filename)\n",
    "                data[\"audio\"].append(filepath)\n",
    "                data[\"labels\"].append(label2id[label])\n",
    "    return data\n",
    "\n",
    "datadict = collect_data()\n",
    "dataset = Dataset.from_dict(datadict, features=features)\n",
    "print(dataset[0], dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4438d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 999/999 [00:00<00:00, 483511.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "dataset = dataset.cast_column(\"labels\", ClassLabel(names=class_names))\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "num_labels = len(np.unique(dataset[\"labels\"]))\n",
    "print(num_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8e30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd730f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "\n",
    "    output_batch = {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}\n",
    "    return output_batch\n",
    "\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "dataset.set_transform(preprocess_audio, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7483b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=0, stratify_by_column=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0caadff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.do_normalize = False\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "for i, (audio_input, labels) in enumerate(dataset[\"train\"]):\n",
    "    cur_mean = torch.mean(dataset[\"train\"][i][audio_input])\n",
    "    cur_std = torch.std(dataset[\"train\"][i][audio_input])\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "\n",
    "feature_extractor.mean = np.mean(mean)\n",
    "feature_extractor.std = np.mean(std)\n",
    "feature_extractor.do_normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2356259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "config.num_labels = num_labels\n",
    "config.label2id = label2id\n",
    "config.id2label = id2label\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba9de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    logging_dir=\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    save_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0958670",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c45aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dded039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 09:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.544654</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.844033</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.832374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.637664</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.845250</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.818392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.744722</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.873713</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.818974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.720271</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.882739</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.867575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.788361</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.879927</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.862045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.638245</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.872594</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.861608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.646170</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.876706</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.866513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.651236</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.876706</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.866513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.653229</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.876706</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.866513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.653944</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.876706</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.866513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.16025200420990587, metrics={'train_runtime': 545.1198, 'train_samples_per_second': 14.657, 'train_steps_per_second': 1.834, 'total_flos': 5.416235474092032e+17, 'train_loss': 0.16025200420990587, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d29b0915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_AST_model\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./saved_AST_model\")\n",
    "\n",
    "def convert_np_floats(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_np_floats(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_np_floats(i) for i in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj\n",
    "config_dict = feature_extractor.to_dict()\n",
    "cleaned_dict = convert_np_floats(config_dict)\n",
    "new_feature_extractor = ASTFeatureExtractor.from_dict(cleaned_dict)\n",
    "new_feature_extractor.save_pretrained(\"./saved_AST_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=\"./logs\"\n",
    "# for stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
